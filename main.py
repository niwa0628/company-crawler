import pandas as pd
from crawler.link_extractor import setup_driver, find_overview_links
from crawler.overview_finder import extract_overview_info
from crawler.utils import save_result_to_csv

EXCEL_PATH = "data/company_list.xlsx"
SHEET_NAME = "èª¿æŸ»ãƒªã‚¹ãƒˆ"
URL_COLUMN = "ä¼æ¥­ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"
OUTPUT_PATH = "data/results.csv"

def load_company_info(excel_path, sheet_name, url_column, date_column="ä½œæ¥­æ—¥"):
    df = pd.read_excel(excel_path, sheet_name=sheet_name)

    # ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ï¼šURLãŒã‚ã£ã¦ã€ä½œæ¥­æ—¥ãŒç©ºç™½
    df_filtered = df[df[url_column].notna() & df[date_column].isna()]

    result = []
    for _, row in df_filtered.iterrows():
        result.append({
            "No": row.get("No", ""),
            "ç¤¾å": row.get("ç¤¾å", ""),
            "URL": str(row[url_column])
        })

    return result



if __name__ == "__main__":
    companies = load_company_info(EXCEL_PATH, SHEET_NAME, URL_COLUMN)
    
    if companies:
        target = companies[0]
        url = target["URL"]

        print(f"[INFO] å‡¦ç†å¯¾è±¡URL: {url}")

        driver = setup_driver(headless=False)
        try:
            links = find_overview_links(driver, url)
            print("[æ¦‚è¦ãƒªãƒ³ã‚¯å€™è£œ]")
            for text, link in links:
                print(f"- {text}: {link}")

            if links:
                first_link = links[0][1]
                print(f"\n[OVERVIEW] {first_link} ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¾ã™...")
                result = extract_overview_info(driver, first_link)

                if result["status"] == "success":
                    print("[OK] æ¦‚è¦æƒ…å ±ã®å–å¾—æˆåŠŸ")
                    print("ğŸ“„ ãƒ†ã‚­ã‚¹ãƒˆæŠœç²‹:")
                    print(result["text_sample"])

                    print("\nğŸ“¦ æŠ½å‡ºçµæœ:")
                    for k, v in result["extracted"].items():
                        print(f"  {k}: {v if v else 'ï¼ˆæŠ½å‡ºã§ããšï¼‰'}")

                else:
                    print("[ERROR] æ¦‚è¦ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—")
                    print(f"ç†ç”±: {result['reason']}")

            else:
                print("[WARN] æ¦‚è¦ãƒªãƒ³ã‚¯å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        finally:
            driver.quit()
    if result["status"] == "success":
        extracted = result["extracted"]
        row = {
             "No": target["No"],
            "ç¤¾å": target["ç¤¾å"],
            "URL": first_link,
            "é›»è©±ç•ªå·": extracted["tel"],
            "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹": extracted["email"],
            "éƒµä¾¿ç•ªå·": extracted["zip"],
            "ä½æ‰€": extracted["address"],
            "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": "success",
            "ã‚¨ãƒ©ãƒ¼ç†ç”±": ""
        }
    else:
        row = {
             "No": target["No"],
            "ç¤¾å": target["ç¤¾å"],
            "URL": first_link,
            "é›»è©±ç•ªå·": "",
            "ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹": "",
            "éƒµä¾¿ç•ªå·": "",
            "ä½æ‰€": "",
            "ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹": "error",
            "ã‚¨ãƒ©ãƒ¼ç†ç”±": result["reason"]
        }
    save_result_to_csv(OUTPUT_PATH, row)
    print(f"[CSV] çµæœã‚’ {OUTPUT_PATH} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")